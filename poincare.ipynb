{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "from hype.sn import Embedding \n",
    "from hype import train\n",
    "from hype.graph import load_edge_list, eval_reconstruction\n",
    "from hype.rsgd import RiemannianSGD\n",
    "from hype.Poincare import PoincareManifold\n",
    "import sys\n",
    "import json\n",
    "import torch.multiprocessing as mp\n",
    "from hype.graph_dataset import BatchedDataset\n",
    "\n",
    "device = torch.device('cpu')\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "#torch.set_default_tensor_type('torch.FloatTensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters; these are global in the notebook!\n",
    "opt_maxnorm = 500000; opt_com_n = 2; opt_debug = False\n",
    "opt_manifold = \"Poincare\"; opt_dim = 2; opt_com_n = 1;\n",
    "opt_negs = 50; opt_batchsize = 10; opt_eval_each = 20;\n",
    "opt_sparse = True; opt_ndproc = 4;  opt_burnin = 20;\n",
    "opt_dampening = 0.75; opt_neg_multiplier = 1.0; \n",
    "opt_burnin_multiplier = 0.01; opt_lr = 0.3 \n",
    "#######################################\n",
    "opt_epochs = 1000; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing logging and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using edge list dataloader\n"
     ]
    }
   ],
   "source": [
    "log_level = logging.DEBUG if opt_debug else logging.INFO\n",
    "log = logging.getLogger('Poincare')\n",
    "logging.basicConfig(level=log_level, format='%(message)s', stream=sys.stdout)\n",
    "log.info('Using edge list dataloader')\n",
    "idx, objects, weights = load_edge_list(\"wordnet/mammal_closure.csv\", False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(manifold, idx, objects, weights, sparse=True):\n",
    "    conf = []\n",
    "    model_name = '%s_dim%d%com_n'\n",
    "    mname = model_name % (opt_manifold, opt_dim, opt_com_n)\n",
    "    data = BatchedDataset(idx, objects, weights, opt_negs, opt_batchsize,\n",
    "        opt_ndproc, opt_burnin > 0, opt_dampening)\n",
    "    model = Embedding(len(data.objects), opt_dim, manifold, sparse=sparse, com_n=opt_com_n)\n",
    "    data.objects = objects\n",
    "    return model, data, mname, conf\n",
    "\n",
    "def adj_matrix(data):\n",
    "  adj = {}\n",
    "  for inputs, _ in data:\n",
    "    for row in inputs:\n",
    "        x = row[0].item(); y = row[1].item()\n",
    "        if x in adj:\n",
    "            adj[x].add(y)\n",
    "        else:\n",
    "            adj[x] = {y}\n",
    "  return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_lr(data, epoch, progress = False):\n",
    "  data.burnin = True \n",
    "  lr = opt_lr\n",
    "  if epoch < opt_burnin:\n",
    "    data.burnin = True\n",
    "    lr = opt_lr * opt_burnin_multiplier\n",
    "  loader_iter = tqdm(data) if progress else data\n",
    "  return loader_iter, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(device, model, data, optimizer, progress=False):\n",
    "  \n",
    "  epoch_loss = torch.Tensor(len(data))\n",
    "  LOSS = np.zeros(opt_epochs)\n",
    "  \n",
    "  for epoch in range(opt_epochs):\n",
    "    largest_weight_emb = round(torch.abs(model.lt.weight.data).max().item(), 6)\n",
    "    print(largest_weight_emb, \"is the largest absolute weight in the embedding\")\n",
    "    \n",
    "    epoch_loss.fill_(0)\n",
    "    t_start = timeit.default_timer()\n",
    "    # handling burnin, get loader_iter and learning rate\n",
    "    loader_iter, lr = data_loader_lr(data, epoch, progress = progress)\n",
    "    \n",
    "    for i_batch, (inputs, targets) in enumerate(loader_iter):\n",
    "      elapsed = timeit.default_timer() - t_start\n",
    "      inputs = inputs.to(device); targets = targets.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      preds = model(inputs)\n",
    "      loss = model.loss(preds, targets, size_average=True)\n",
    "      loss.backward()\n",
    "      optimizer.step(lr=lr)\n",
    "      epoch_loss[i_batch] = loss.cpu().item()\n",
    "    LOSS[epoch] = torch.mean(epoch_loss).item()\n",
    "    # since only one thread is used:\n",
    "    log.info('json_stats: {' f'\"epoch\": {epoch}, ' \\\n",
    "    f'\"elapsed\": {elapsed}, ' f'\"loss\": {LOSS[epoch]}, ' '}')\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>> The size of embedding: 1180 and 2 ;and sparse: True\n",
      "the total dimension 2 com_n 1\n",
      "0.0001 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 0, \"elapsed\": 0.8434281080008077, \"loss\": 3.93100562588228, }\n",
      "0.004062 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 1, \"elapsed\": 0.8734509070018248, \"loss\": 3.9290048422196553, }\n",
      "0.007878 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 2, \"elapsed\": 0.854821629000071, \"loss\": 3.9269469856998818, }\n",
      "0.011601 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 3, \"elapsed\": 0.8515651689995138, \"loss\": 3.9248984785878283, }\n",
      "0.015237 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 4, \"elapsed\": 0.8563593250000849, \"loss\": 3.9228398701244616, }\n",
      "0.018724 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 5, \"elapsed\": 0.8582052629972168, \"loss\": 3.920792682067316, }\n",
      "0.022219 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 6, \"elapsed\": 0.8578112089999195, \"loss\": 3.9187733247780807, }\n",
      "0.025715 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 7, \"elapsed\": 0.8560112160012068, \"loss\": 3.916722467409325, }\n",
      "0.029309 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 8, \"elapsed\": 0.8642718830014928, \"loss\": 3.9146885375784986, }\n",
      "0.032838 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 9, \"elapsed\": 0.8693568169983337, \"loss\": 3.912662332500639, }\n",
      "0.036327 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 10, \"elapsed\": 0.8648264309995284, \"loss\": 3.9106252096922534, }\n",
      "0.03982 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 11, \"elapsed\": 0.8800811149994843, \"loss\": 3.9086124355876994, }\n",
      "0.043297 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 12, \"elapsed\": 0.863797087000421, \"loss\": 3.9065960042138452, }\n",
      "0.046754 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 13, \"elapsed\": 0.8644613180003944, \"loss\": 3.9046167958343276, }\n",
      "0.050195 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 14, \"elapsed\": 0.8603883329997188, \"loss\": 3.902640269935824, }\n",
      "0.053618 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 15, \"elapsed\": 0.8558661780007242, \"loss\": 3.900633630065862, }\n",
      "0.05704 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 16, \"elapsed\": 0.8562583910024841, \"loss\": 3.8986468071324523, }\n",
      "0.06043 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 17, \"elapsed\": 0.8677567990016541, \"loss\": 3.8967089338331755, }\n",
      "0.063737 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 18, \"elapsed\": 0.8562308799992024, \"loss\": 3.894689794223632, }\n",
      "0.067056 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 19, \"elapsed\": 0.8629332099990279, \"loss\": 3.8928126870816078, }\n",
      "0.070397 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 20, \"elapsed\": 0.8549627709980996, \"loss\": 3.82616175108882, }\n",
      "0.332472 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 21, \"elapsed\": 0.8447107579995645, \"loss\": 3.6776209078094833, }\n",
      "0.501153 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 22, \"elapsed\": 0.8733884819994273, \"loss\": 3.5520914842966627, }\n",
      "0.601305 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 23, \"elapsed\": 0.851410888000828, \"loss\": 3.4380817232934966, }\n",
      "0.685313 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 24, \"elapsed\": 0.8650322319990664, \"loss\": 3.3345911546650733, }\n",
      "0.738551 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 25, \"elapsed\": 0.8566364500002237, \"loss\": 3.239965427474321, }\n",
      "0.779304 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 26, \"elapsed\": 0.8649163450027118, \"loss\": 3.146909473389324, }\n",
      "0.823661 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 27, \"elapsed\": 0.8689639969998098, \"loss\": 3.0620508431598954, }\n",
      "0.859655 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 28, \"elapsed\": 0.8629260439993232, \"loss\": 2.976412683544098, }\n",
      "0.885507 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 29, \"elapsed\": 0.8621326630018302, \"loss\": 2.8994450899764845, }\n",
      "0.906546 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 30, \"elapsed\": 0.850961742002255, \"loss\": 2.8288780542126672, }\n",
      "0.923725 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 31, \"elapsed\": 0.8628527800028678, \"loss\": 2.762093034428678, }\n",
      "0.934903 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 32, \"elapsed\": 0.8570094649985549, \"loss\": 2.6955534039369446, }\n",
      "0.945623 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 33, \"elapsed\": 0.8685272809998423, \"loss\": 2.6342666745026158, }\n",
      "0.952233 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 34, \"elapsed\": 0.8537592840002617, \"loss\": 2.5744012087167834, }\n",
      "0.960283 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 35, \"elapsed\": 0.8562026129984588, \"loss\": 2.52355999294797, }\n",
      "0.966683 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 36, \"elapsed\": 0.8605615209999087, \"loss\": 2.4689665558838167, }\n",
      "0.972039 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 37, \"elapsed\": 0.8578121089994966, \"loss\": 2.418689303145859, }\n",
      "0.975865 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 38, \"elapsed\": 0.8512755459996697, \"loss\": 2.3699690240099445, }\n",
      "0.979507 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 39, \"elapsed\": 0.8541912979999324, \"loss\": 2.3261897955118007, }\n",
      "0.983346 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 40, \"elapsed\": 0.8561206850026792, \"loss\": 2.2801031609269304, }\n",
      "0.985235 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 41, \"elapsed\": 0.8574687050022476, \"loss\": 2.24396425164607, }\n",
      "0.986977 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 42, \"elapsed\": 0.8660276810005598, \"loss\": 2.198303745085496, }\n",
      "0.988981 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 43, \"elapsed\": 0.8663316340025631, \"loss\": 2.168862197057914, }\n",
      "0.990636 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 44, \"elapsed\": 0.8564345209997555, \"loss\": 2.1270785356753086, }\n",
      "0.992107 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 45, \"elapsed\": 0.8869792570003483, \"loss\": 2.0824073939674483, }\n",
      "0.992982 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 46, \"elapsed\": 0.8699533879989758, \"loss\": 2.0579241525791176, }\n",
      "0.99359 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 47, \"elapsed\": 0.8672477039981459, \"loss\": 2.0122754591400165, }\n",
      "0.994241 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 48, \"elapsed\": 0.8666731800003618, \"loss\": 1.9882553673929984, }\n",
      "0.994753 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 49, \"elapsed\": 0.8671949439994933, \"loss\": 1.9573136105068787, }\n",
      "0.995452 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 50, \"elapsed\": 0.8746557859994937, \"loss\": 1.9311446372015264, }\n",
      "0.995608 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 51, \"elapsed\": 0.8661255549995985, \"loss\": 1.9044676567019359, }\n",
      "0.996164 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 52, \"elapsed\": 0.8617339350021211, \"loss\": 1.8705393562268953, }\n",
      "0.996549 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 53, \"elapsed\": 0.8823134379999829, \"loss\": 1.846721930540538, }\n",
      "0.996924 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 54, \"elapsed\": 0.8621867149995524, \"loss\": 1.8209975289470568, }\n",
      "0.997235 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 55, \"elapsed\": 0.8588424980007403, \"loss\": 1.7941310060512092, }\n",
      "0.997495 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 56, \"elapsed\": 0.8553094070011866, \"loss\": 1.7727332605682116, }\n",
      "0.997663 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 57, \"elapsed\": 0.8532807490009873, \"loss\": 1.7498658160131115, }\n",
      "0.99776 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 58, \"elapsed\": 0.8627991729990754, \"loss\": 1.7243917210933482, }\n",
      "0.997952 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 59, \"elapsed\": 0.8497301429997606, \"loss\": 1.705245314833798, }\n",
      "0.998124 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 60, \"elapsed\": 0.8470153760026733, \"loss\": 1.690345987722803, }\n",
      "0.998254 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 61, \"elapsed\": 0.8583821479987819, \"loss\": 1.6592274254063997, }\n",
      "0.998433 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 62, \"elapsed\": 0.8592049430008046, \"loss\": 1.6545891667849706, }\n",
      "0.998559 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 63, \"elapsed\": 0.8847130760004802, \"loss\": 1.632116297631658, }\n",
      "0.998566 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 64, \"elapsed\": 0.862833387000137, \"loss\": 1.61305290110502, }\n",
      "0.998621 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 65, \"elapsed\": 0.8569374319995404, \"loss\": 1.5877247736149882, }\n",
      "0.998698 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 66, \"elapsed\": 0.8503114359991741, \"loss\": 1.589712838027728, }\n",
      "0.998791 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 67, \"elapsed\": 0.8552906669974618, \"loss\": 1.5711029448744684, }\n",
      "0.998872 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 68, \"elapsed\": 0.854033129002346, \"loss\": 1.547765669799675, }\n",
      "0.998997 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 69, \"elapsed\": 0.8680712780005706, \"loss\": 1.5381668423509145, }\n",
      "0.999046 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 70, \"elapsed\": 0.872478247001709, \"loss\": 1.5311522353644955, }\n",
      "0.999173 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 71, \"elapsed\": 0.8739752729998145, \"loss\": 1.5211204029060543, }\n",
      "0.999255 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 72, \"elapsed\": 0.8731473260013445, \"loss\": 1.5001175034843222, }\n",
      "0.999276 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 73, \"elapsed\": 0.897176190999744, \"loss\": 1.5015276694536253, }\n",
      "0.999347 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 74, \"elapsed\": 0.875169664999703, \"loss\": 1.484185759506116, }\n",
      "0.999377 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 75, \"elapsed\": 0.8753318800008856, \"loss\": 1.470792565863611, }\n",
      "0.999416 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 76, \"elapsed\": 0.878394922001462, \"loss\": 1.4649036950535506, }\n",
      "0.999421 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 77, \"elapsed\": 0.8722638700019161, \"loss\": 1.4523578526120067, }\n",
      "0.999441 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 78, \"elapsed\": 0.8638140819966793, \"loss\": 1.4407452621114543, }\n",
      "0.999486 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 79, \"elapsed\": 0.8811196479982755, \"loss\": 1.4356923382402966, }\n",
      "0.999485 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 80, \"elapsed\": 0.8591676530013501, \"loss\": 1.4239507947018748, }\n",
      "0.99949 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 81, \"elapsed\": 0.8490545940003358, \"loss\": 1.4186645356139813, }\n",
      "0.999538 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 82, \"elapsed\": 0.8533219040000404, \"loss\": 1.4107127826814432, }\n",
      "0.999586 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 83, \"elapsed\": 0.8653330340021057, \"loss\": 1.4002359041331698, }\n",
      "0.999617 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 84, \"elapsed\": 0.8808706039999379, \"loss\": 1.3896737062318787, }\n",
      "0.99964 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 85, \"elapsed\": 0.8661294800003816, \"loss\": 1.3853126618717542, }\n",
      "0.999664 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 86, \"elapsed\": 0.8368299059984565, \"loss\": 1.3855959430832245, }\n",
      "0.999669 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 87, \"elapsed\": 0.9042618890016456, \"loss\": 1.3772244908485212, }\n",
      "0.999693 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 88, \"elapsed\": 0.8652752859998145, \"loss\": 1.3589177006117414, }\n",
      "0.999706 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 89, \"elapsed\": 0.8589630569986184, \"loss\": 1.3592135425524643, }\n",
      "0.999715 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 90, \"elapsed\": 0.855462137998984, \"loss\": 1.3470053052716995, }\n",
      "0.999741 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 91, \"elapsed\": 0.866221129999758, \"loss\": 1.3402170088089125, }\n",
      "0.999749 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 92, \"elapsed\": 0.8631525140008307, \"loss\": 1.3405072522650507, }\n",
      "0.999757 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 93, \"elapsed\": 0.861776630001259, \"loss\": 1.3364823096269616, }\n",
      "0.999774 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 94, \"elapsed\": 0.8708397399968817, \"loss\": 1.3211306716137416, }\n",
      "0.999784 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 95, \"elapsed\": 0.8788847559990245, \"loss\": 1.3252367820633693, }\n",
      "0.999792 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 96, \"elapsed\": 0.8730690459997277, \"loss\": 1.3189672002144923, }\n",
      "0.999811 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 97, \"elapsed\": 0.879489859999012, \"loss\": 1.3120109835492644, }\n",
      "0.999822 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 98, \"elapsed\": 0.8575621109994245, \"loss\": 1.3067352858137808, }\n",
      "0.999827 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 99, \"elapsed\": 0.8520428140000149, \"loss\": 1.302339081401216, }\n",
      "0.999833 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 100, \"elapsed\": 0.8532468140001583, \"loss\": 1.2983150395491923, }\n",
      "0.999841 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 101, \"elapsed\": 0.8564887850006926, \"loss\": 1.2982457096350193, }\n",
      "0.99985 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 102, \"elapsed\": 0.860898429997178, \"loss\": 1.2908257725593633, }\n",
      "0.999851 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 103, \"elapsed\": 0.8664628740007174, \"loss\": 1.2910353813990547, }\n",
      "0.999864 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 104, \"elapsed\": 0.8762321140020504, \"loss\": 1.2818661017903068, }\n",
      "0.999866 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 105, \"elapsed\": 0.8663619749968348, \"loss\": 1.2883636440461588, }\n",
      "0.999872 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 106, \"elapsed\": 0.8545584380008222, \"loss\": 1.2711317504438524, }\n",
      "0.999876 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 107, \"elapsed\": 0.8746226809998916, \"loss\": 1.27853662178875, }\n",
      "0.999885 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 108, \"elapsed\": 0.8677778729979764, \"loss\": 1.2768207594736745, }\n",
      "0.999889 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 109, \"elapsed\": 0.8635985879991495, \"loss\": 1.269212988403335, }\n",
      "0.999889 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 110, \"elapsed\": 0.8533591190025618, \"loss\": 1.2579802041533639, }\n",
      "0.999895 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 111, \"elapsed\": 0.8590209229987522, \"loss\": 1.2568502575283058, }\n",
      "0.999902 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 112, \"elapsed\": 0.8575350449973485, \"loss\": 1.2505609874400165, }\n",
      "0.999905 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 113, \"elapsed\": 0.8920931879983982, \"loss\": 1.2568957491550818, }\n",
      "0.999908 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 114, \"elapsed\": 0.8959371170021768, \"loss\": 1.2471331086352517, }\n",
      "0.99991 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 115, \"elapsed\": 0.871183341001597, \"loss\": 1.2445232476095585, }\n",
      "0.999917 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 116, \"elapsed\": 0.8525548910001817, \"loss\": 1.2379186141125909, }\n",
      "0.999921 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 117, \"elapsed\": 0.8679943729985098, \"loss\": 1.2441703593218896, }\n",
      "0.999923 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 118, \"elapsed\": 0.8468870340002468, \"loss\": 1.2385491978219745, }\n",
      "0.999923 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 119, \"elapsed\": 0.8557499029993778, \"loss\": 1.2298890246337597, }\n",
      "0.999925 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 120, \"elapsed\": 0.8646865669979888, \"loss\": 1.2407911261900406, }\n",
      "0.999926 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 121, \"elapsed\": 0.856906209999579, \"loss\": 1.2319643148891237, }\n",
      "0.999929 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 122, \"elapsed\": 0.8501110279976274, \"loss\": 1.226948719826956, }\n",
      "0.99993 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 123, \"elapsed\": 0.9125725060002878, \"loss\": 1.2216990375844219, }\n",
      "0.999932 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 124, \"elapsed\": 0.8762566709992825, \"loss\": 1.2226682011435552, }\n",
      "0.999932 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 125, \"elapsed\": 0.8666987430005975, \"loss\": 1.217605390217558, }\n",
      "0.999933 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 126, \"elapsed\": 0.8614572120022785, \"loss\": 1.2262569356907984, }\n",
      "0.999935 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 127, \"elapsed\": 0.8510683820022678, \"loss\": 1.2157411063997654, }\n",
      "0.999937 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 128, \"elapsed\": 0.870836201000202, \"loss\": 1.2110937713641579, }\n",
      "0.999938 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 129, \"elapsed\": 0.8503689040007885, \"loss\": 1.2168644934394606, }\n",
      "0.999939 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 130, \"elapsed\": 0.8602760409994517, \"loss\": 1.2135228693275766, }\n",
      "0.99994 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 131, \"elapsed\": 0.8602835919991776, \"loss\": 1.2076867397330386, }\n",
      "0.999943 is the largest absolute weight in the embedding\n",
      "json_stats: {\"epoch\": 132, \"elapsed\": 0.8564588859990181, \"loss\": 1.2041963176874435, }\n",
      "0.999945 is the largest absolute weight in the embedding\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f0cc96fda765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# begin training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-94a85d748467>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, model, data, optimizer, progress)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mepoch_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MCF/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MCF/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "manifold = PoincareManifold(\n",
    "    debug=opt_debug, max_norm=opt_maxnorm, com_n=opt_com_n)\n",
    "model, data, model_name, conf = init_model(\n",
    "    manifold, idx, objects, weights, sparse=opt_sparse)\n",
    "data.neg_multiplier = opt_neg_multiplier\n",
    "model = model.to(device)\n",
    "print('the total dimension', model.lt.weight.data.size(-1), 'com_n', opt_com_n)\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = RiemannianSGD(model.optim_params(manifold), lr= opt_lr)\n",
    "# get adjacency matrix\n",
    "adj = adj_matrix(data) \n",
    "# begin training\n",
    "start_time = timeit.default_timer()\n",
    "train(device, model, data, optimizer, progress=False )\n",
    "print(\"Total training time is:\", timeit.default_timer() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_stats final test: \n",
      "{\"sqnorm_min\": 0.049918, \"sqnorm_avg\": 0.87429, \"sqnorm_max\": 0.999998, \n",
      "\"mean_rank\": 434.161774, \"map\": 0.016397, }\n"
     ]
    }
   ],
   "source": [
    "meanrank, maprank = eval_reconstruction(\n",
    "    adj, model.lt.weight.data.clone(), manifold.distance, workers=opt_ndproc)\n",
    "sqnorms = manifold.pnorm(model.lt.weight.data.clone())\n",
    "log.info(\n",
    "        'json_stats final test: \\n{' \n",
    "        f'\"sqnorm_min\": {round(sqnorms.min().item(),6)}, '\n",
    "        f'\"sqnorm_avg\": {round(sqnorms.mean().item(),6)}, '\n",
    "        f'\"sqnorm_max\": {round(sqnorms.max().item(),6)}, \\n'\n",
    "        f'\"mean_rank\": {round(meanrank,6)}, '\n",
    "        f'\"map\": {round(maprank,6)}, '\n",
    "        '}'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a0267f670e2b0941e569fb4138371a40e75b428fb955a02da7bcb7cd596bc5c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
